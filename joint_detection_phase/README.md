# SHS_NET joint detection phase
The joint detection phase aims to identify the joint coordinates in the image by weakly supervised learning using heat-mapped images based on U-Net. An overview of this phase follows.

![joint_detection2](https://user-images.githubusercontent.com/80377824/171571670-1247f528-e6bf-451d-98e1-0bbd45195f6c.png)


# Code Architecture
<pre>
.　　
├── data_set                               # hand radiographs for training.   
│   ├──images                              # Directory to store hand radiographs generated by orientation model. Please copy the images generated in the output directory of orientation_phase to this directory.
│   ├──masks                               # Directory to store heatmaps generated by heatmap_generator.ipynb
│   │  ├──0                                # Directory where the heatmap image of joint number 0 (IP joint in this model) is stored
│   │  ├──1                                # Directory where the heatmap image of joint number 1 (PIP2 joint in this model) is stored
│   │    ...
│   ├──test                                # Directory to store hand radiographs for testing and each joint image that was cropped 
│   └──coord_list.csv                      # Correct label list of 15 joint coordinates for each hand radiograph. This list is used in heatmap_generator.ipynb to generate a heatmap from the joint coordinates                    
├── models                      
│   └── model name                         # Directory to store config, log and weight parameter files               
├── README.md                              # README file   
├── U-Net_base_joint_coord_detector.ipynb  # main code of joint detection phase  
├── archs.py                               # code for architecuture of U-Net  
├── dataset.py                             # code for making data-loader from images in hand_all_rotation dir and image_list_hand_ver3.csv  
├── heatmap_generator.ipynb                # code for generating heatmaps
├── losses.py                              # code for loss function  
├── metrics.py                             # code for culcurate SDR(Standard dimension ratio)
├── train_val.py                           # code for training and validation  
└── utils.py                               # common useful modules (to make scheduler, optimizer, label maker for training and validation etc.)  
</pre> 

This repository contains dummy images obtained from [RSNA-Pediatric-Bone-Age-Challenge-2017](https://www.rsna.org/education/ai-resources-and-training/ai-image-challenge/RSNA-Pediatric-Bone-Age-Challenge-2017)   
Halabi SS, Prevedello LM, Kalpathy-Cramer J, et al. The RSNA Pediatric Bone Age Machine Learning Challenge. Radiology 2018; 290(2):498-503.

code of adabound can be found [here](https://github.com/Luolc/AdaBound)
